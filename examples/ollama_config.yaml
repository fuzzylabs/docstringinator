# Example configuration for using Ollama with Docstringinator
# 
# 1. Install Ollama: https://ollama.ai/download
# 2. Pull a model: ollama pull llama2
# 3. Start Ollama: ollama serve
# 4. Use this configuration

llm:
  provider: "ollama"
  model: "llama2"  # or "codellama", "mistral", etc.
  base_url: "http://localhost:11434"  # Default Ollama URL
  temperature: 0.1
  timeout: 30

format:
  style: "google"  # google, numpy, restructuredtext
  include_examples: true
  include_type_hints: true
  max_line_length: 88
  include_raises: true
  include_returns: true

processing:
  dry_run: false
  backup_files: true
  max_file_size: 1000000  # bytes
  exclude_patterns:
    - "*/tests/*"
    - "*/migrations/*"
    - "*/venv/*"
    - "*/__pycache__/*"
    - "*/build/*"
    - "*/dist/*"
  include_patterns:
    - "*.py"

output:
  verbose: true
  show_diff: true
  create_backup: true
  output_format: "text" 